{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lyrics Clustering Pipeline\n",
    "This notebook replicates the functionality of `lyrics_clustering.py` so the analysis can be run step by step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "PUNCT_TABLE = str.maketrans('', '', string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag: str) -> str:\n",
    "    tag = tag[0].upper()\n",
    "    return {\n",
    "        'J': 'a',\n",
    "        'N': 'n',\n",
    "        'V': 'v',\n",
    "        'R': 'r',\n",
    "    }.get(tag, 'n')\n",
    "\n",
    "\n",
    "def preprocess(text: str, method: str = 'lemma', ngram: int = 1) -> str:\n",
    "    tokens = [t.lower() for t in word_tokenize(text)]\n",
    "    tokens = [t.translate(PUNCT_TABLE) for t in tokens]\n",
    "    tokens = [t for t in tokens if t and t not in STOP_WORDS]\n",
    "    if method == 'stem':\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "    else:\n",
    "        lemm = WordNetLemmatizer()\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        tokens = [lemm.lemmatize(t, get_wordnet_pos(p)) for t, p in pos_tags]\n",
    "    if ngram > 1:\n",
    "        grams = [' '.join(tokens[i:i + ngram]) for i in range(len(tokens) - ngram + 1)]\n",
    "    else:\n",
    "        grams = tokens\n",
    "    return ' '.join(grams)\n",
    "\n",
    "\n",
    "def load_lyrics(csv_path: Path) -> pd.Series:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if 'Lyric' not in df.columns:\n",
    "        raise ValueError('Expected column \"Lyric\" in dataset')\n",
    "    df = df.dropna(subset=['Lyric'])\n",
    "    return df['Lyric']\n",
    "\n",
    "\n",
    "def embed_lyrics(lyrics: list[str], model_name: str) -> np.ndarray:\n",
    "    model = SentenceTransformer(model_name)\n",
    "    embeddings = model.encode(lyrics, show_progress_bar=True)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "\n",
    "def run_kmeans(data: np.ndarray, k: int):\n",
    "    km = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = km.fit_predict(data)\n",
    "    score = silhouette_score(data, labels)\n",
    "    return labels, score\n",
    "\n",
    "\n",
    "def run_dbscan(data: np.ndarray, eps: float, min_samples: int):\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = db.fit_predict(data)\n",
    "    if len(set(labels)) > 1 and -1 not in set(labels):\n",
    "        score = silhouette_score(data, labels)\n",
    "    else:\n",
    "        score = float('nan')\n",
    "    return labels, score\n",
    "\n",
    "\n",
    "def run_agglomerative(data: np.ndarray, k: int):\n",
    "    agg = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "    labels = agg.fit_predict(data)\n",
    "    score = silhouette_score(data, labels)\n",
    "    return labels, score\n",
    "\n",
    "\n",
    "def elbow_plot(data: np.ndarray, k_range: range, out_path: Path):\n",
    "    sse = []\n",
    "    for k in k_range:\n",
    "        km = KMeans(n_clusters=k, random_state=42)\n",
    "        km.fit(data)\n",
    "        sse.append(km.inertia_)\n",
    "    plt.figure()\n",
    "    plt.plot(list(k_range), sse, marker='o')\n",
    "    plt.xlabel('Number of clusters (k)')\n",
    "    plt.ylabel('SSE')\n",
    "    plt.title('Elbow Method for K-Means')\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def scatter_plot(data: np.ndarray, labels: np.ndarray, out_path: Path):\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    coords = pca.fit_transform(data)\n",
    "    plt.figure()\n",
    "    num_labels = len(set(labels))\n",
    "    for lab in set(labels):\n",
    "        idx = labels == lab\n",
    "        plt.scatter(coords[idx, 0], coords[idx, 1], label=str(lab), s=10)\n",
    "    if num_labels <= 10:\n",
    "        plt.legend()\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def summarise_dataset(lyrics: pd.Series) -> dict:\n",
    "    lengths = lyrics.str.split().str.len()\n",
    "    return {\n",
    "        'num_songs': len(lyrics),\n",
    "        'min_length': lengths.min(),\n",
    "        'max_length': lengths.max(),\n",
    "        'avg_length': lengths.mean(),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = Path('musicLyrics.csv')\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "preprocess_method = 'lemma'  # or 'stem'\n",
    "ngram_size = 1\n",
    "output_dir = Path('outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = load_lyrics(csv_path)\n",
    "summary = summarise_dataset(lyrics)\n",
    "print(f\"Loaded {summary['num_songs']} songs\")\n",
    "print(f\"Average length: {summary['avg_length']:.1f} words\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = [preprocess(t, method=preprocess_method, ngram=ngram_size) for t in tqdm(lyrics, desc='Preprocess')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embed_lyrics(processed, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_plot(embeddings, range(2, 8), output_dir / 'kmeans_elbow.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_labels, k_score = run_kmeans(embeddings, k=2)\n",
    "scatter_plot(embeddings, k_labels, output_dir / 'kmeans_scatter.png')\n",
    "\n",
    "db_labels, db_score = run_dbscan(embeddings, eps=0.5, min_samples=5)\n",
    "if not np.isnan(db_score):\n",
    "    scatter_plot(embeddings, db_labels, output_dir / 'dbscan_scatter.png')\n",
    "\n",
    "ag_labels, ag_score = run_agglomerative(embeddings, k=3)\n",
    "scatter_plot(embeddings, ag_labels, output_dir / 'agg_scatter.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\n",
    "Silhouette Scores:')\n",
    "print(f'  K-Means (k=2): {k_score:.3f}')\n",
    "print(f'  DBSCAN: {db_score:.3f}')\n",
    "print(f'  Agglomerative (k=3): {ag_score:.3f}')\n",
    "\n",
    "examples = defaultdict(list)\n",
    "for lyric, lab in zip(lyrics, k_labels):\n",
    "    if len(examples[lab]) < 2:\n",
    "        examples[lab].append(lyric[:120] + '...')\n",
    "    if all(len(v) >= 2 for v in examples.values()):\n",
    "        break\n",
    "\n",
    "print('\n",
    "Cluster Examples (K-Means):')\n",
    "for lab, exs in examples.items():\n",
    "    print(f'Cluster {lab}')\n",
    "    for ex in exs:\n",
    "        print(f'  - {ex}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}