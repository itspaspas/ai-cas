{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bbcf9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\itspa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from sklearn.metrics import homogeneity_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d76ade21",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = Path(\"outputs\")\n",
    "fig_dir = out_dir / \"figures\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a85ed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "PUNCTUATION = set(string.punctuation)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nltk_tag_to_wordnet(nltk_tag: str):\n",
    "    if nltk_tag.startswith(\"J\"):\n",
    "        return \"a\"\n",
    "    if nltk_tag.startswith(\"V\"):\n",
    "        return \"v\"\n",
    "    if nltk_tag.startswith(\"N\"):\n",
    "        return \"n\"\n",
    "    if nltk_tag.startswith(\"R\"):\n",
    "        return \"r\"\n",
    "    return \"n\"\n",
    "\n",
    "\n",
    "def clean_and_tokenise(text: str):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in STOPWORDS]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def stem_pipeline(texts):\n",
    "    return [\" \".join(stemmer.stem(t) for t in clean_and_tokenise(doc)) for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatise_pipeline(texts):\n",
    "    processed = []\n",
    "    for doc in texts:\n",
    "        tokens = clean_and_tokenise(doc)\n",
    "        if not tokens:\n",
    "            processed.append(\"\")\n",
    "            continue\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        lemmas = [lemmatizer.lemmatize(t, nltk_tag_to_wordnet(p)) for t, p in pos_tags]\n",
    "        processed.append(\" \".join(lemmas))\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "813a943e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2,999 lyrics.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('musicLyrics.csv')\n",
    "print(f\"Loaded {len(df):,} lyrics.\")\n",
    "\n",
    "raw_texts = df[\"Lyric\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61f847e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\itspa\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63557e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stemming pipeline …\n",
      "Running lemmatisation pipeline …\n"
     ]
    }
   ],
   "source": [
    "print(\"Running stemming pipeline …\")\n",
    "stemmed_texts = stem_pipeline(raw_texts)\n",
    "\n",
    "print(\"Running lemmatisation pipeline …\")\n",
    "lemm_texts = lemmatise_pipeline(raw_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79e3e1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings (stemming) …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a368c0690d35419eb3059f21a6d458ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings (lemmatisation) …\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0caaa1662aef4d728e9abb0f7fd1fe55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "print(\"Computing embeddings (stemming) …\")\n",
    "embeddings_stem = model.encode(stemmed_texts, batch_size=64, show_progress_bar=True)\n",
    "\n",
    "print(\"Computing embeddings (lemmatisation) …\")\n",
    "embeddings_lem = model.encode(lemm_texts, batch_size=64, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d8e366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_and_silhouette(embeddings, title_prefix):\n",
    "    distortions = []\n",
    "    silhouettes = []\n",
    "    K_range = range(2, 16)\n",
    "    for k in tqdm(K_range, desc=\"K‑Means sweep\"):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(embeddings)\n",
    "        distortions.append(kmeans.inertia_)\n",
    "        silhouettes.append(silhouette_score(embeddings, labels))\n",
    "\n",
    "    fig1 = plt.figure()\n",
    "    plt.plot(list(K_range), distortions, marker=\"o\")\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Inertia (sum of squared distances)\")\n",
    "    plt.title(f\"{title_prefix} | Elbow method\")\n",
    "    fig1.savefig(fig_dir / f\"{title_prefix.lower().replace(' ', '_')}_elbow.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig1)\n",
    "\n",
    "    fig2 = plt.figure()\n",
    "    plt.plot(list(K_range), silhouettes, marker=\"o\")\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Silhouette score\")\n",
    "    plt.title(f\"{title_prefix} | Silhouette vs. K\")\n",
    "    fig2.savefig(fig_dir / f\"{title_prefix.lower().replace(' ', '_')}_silhouette.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig2)\n",
    "\n",
    "    best_k = int(np.argmax(silhouettes) + 2)\n",
    "    return best_k, silhouettes[best_k - 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4d55aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5c0c2bc33b43469635503fe9c75cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "K‑Means sweep:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d76ce13394493a91e840b97150746f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "K‑Means sweep:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best K (stemming): 2 | silhouette = 0.056\n",
      "Best K (lemmatisation): 2 | silhouette = 0.063\n"
     ]
    }
   ],
   "source": [
    "k_stem, sil_stem = elbow_and_silhouette(embeddings_stem, \"Stemming\")\n",
    "k_lem, sil_lem = elbow_and_silhouette(embeddings_lem, \"Lemmatisation\")\n",
    "\n",
    "print(f\"Best K (stemming): {k_stem} | silhouette = {sil_stem:.3f}\")\n",
    "print(f\"Best K (lemmatisation): {k_lem} | silhouette = {sil_lem:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1efa56e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Continuing with the lemmatisation pipeline …\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_embeddings = embeddings_lem if sil_lem >= sil_stem else embeddings_stem\n",
    "pipeline_name = \"lemmatisation\" if sil_lem >= sil_stem else \"stemming\"\n",
    "\n",
    "print(f\"\\nContinuing with the {pipeline_name} pipeline …\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61cf80fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_best = k_lem if sil_lem >= sil_stem else k_stem\n",
    "kmeans = KMeans(n_clusters=k_best, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(use_embeddings)\n",
    "kmeans_sil = silhouette_score(use_embeddings, kmeans_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce8b8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_results = {}\n",
    "for eps in np.linspace(0.5, 5.0, 10):\n",
    "    db = DBSCAN(eps=eps, min_samples=5, metric=\"cosine\").fit(use_embeddings)\n",
    "    labels = db.labels_\n",
    "    unique_labels = set(labels)\n",
    "    if len(unique_labels) <= 1:\n",
    "        continue\n",
    "    sil = silhouette_score(use_embeddings, labels)\n",
    "    dbscan_results[eps] = sil\n",
    "if dbscan_results:\n",
    "    best_eps = max(dbscan_results, key=dbscan_results.get)\n",
    "    dbscan = DBSCAN(eps=best_eps, min_samples=5, metric=\"cosine\")\n",
    "    dbscan_labels = dbscan.fit_predict(use_embeddings)\n",
    "    dbscan_sil = dbscan_results[best_eps]\n",
    "else:\n",
    "    best_eps = None\n",
    "    dbscan_labels = None\n",
    "    dbscan_sil = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c6dad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sil_agg = -1\n",
    "best_k_agg = None\n",
    "best_labels_agg = None\n",
    "for k in range(2, 16):\n",
    "    agg = AgglomerativeClustering(n_clusters=k, linkage=\"ward\")\n",
    "    labels = agg.fit_predict(use_embeddings)\n",
    "    sil = silhouette_score(use_embeddings, labels)\n",
    "    if sil > best_sil_agg:\n",
    "        best_sil_agg = sil\n",
    "        best_k_agg = k\n",
    "        best_labels_agg = labels\n",
    "agg_sil = best_sil_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "433c679e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, random_state=42)\n",
    "embeddings_2d = pca.fit_transform(use_embeddings)\n",
    "\n",
    "def plot_clusters(labels, title, filename):\n",
    "    fig = plt.figure(figsize=(6, 5))\n",
    "    scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels, s=8, alpha=0.7)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"PCA‑1\")\n",
    "    plt.ylabel(\"PCA‑2\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(fig_dir / filename, dpi=300)\n",
    "    plt.close(fig)\n",
    "\n",
    "plot_clusters(kmeans_labels, f\"K‑Means (K={k_best})\", f\"clusters_kmeans_k{k_best}.png\")\n",
    "if dbscan_labels is not None:\n",
    "    plot_clusters(dbscan_labels, f\"DBSCAN (eps={best_eps:.2f})\", f\"clusters_dbscan_eps{best_eps:.2f}.png\")\n",
    "plot_clusters(best_labels_agg, f\"Agglomerative (K={best_k_agg})\", f\"clusters_agg_k{best_k_agg}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94133a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_cluster = defaultdict(list)\n",
    "for idx, (cluster_id, lyric) in enumerate(zip(kmeans_labels, raw_texts)):\n",
    "    if len(examples_per_cluster[cluster_id]) < 3:\n",
    "        examples_per_cluster[cluster_id].append(lyric[:400].strip().replace(\"\\n\", \" \") + \"…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b040e3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved visualisations in outputs\\figures/ and report at outputs\\report.md\n"
     ]
    }
   ],
   "source": [
    "report_path = out_dir / \"report.md\"\n",
    "with report_path.open(\"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(\"# Lyrics Clustering Report\\n\\n\")\n",
    "    fp.write(f\"**Dataset size:** {len(df):,} songs\\n\\n\")\n",
    "    fp.write(\"## Pre‑processing Comparison\\n\")\n",
    "    fp.write(f\"* Stemming pipeline — best K‑Means silhouette = {sil_stem:.3f} at K = {k_stem}\\n\")\n",
    "    fp.write(f\"* Lemmatisation pipeline — best K‑Means silhouette = {sil_lem:.3f} at K = {k_lem}\\n\\n\")\n",
    "    fp.write(f\"We proceeded with **{pipeline_name}** because it yielded the higher silhouette score.\\n\\n\")\n",
    "\n",
    "    fp.write(\"## Final Clustering Scores\\n\")\n",
    "    fp.write(\"| Algorithm | #Clusters | Silhouette | Notes |\\n\")\n",
    "    fp.write(\"|-----------|-----------|------------|-------|\\n\")\n",
    "    fp.write(f\"| K‑Means | {k_best} | {kmeans_sil:.3f} | elbow + silhouette selected |\\n\")\n",
    "    if dbscan_labels is not None:\n",
    "        fp.write(f\"| DBSCAN | {len(set(dbscan_labels))- (1 if -1 in dbscan_labels else 0)} | {dbscan_sil:.3f} | eps = {best_eps:.2f} |\\n\")\n",
    "    else:\n",
    "        fp.write(\"| DBSCAN | — | — | failed to find >1 cluster |\\n\")\n",
    "    fp.write(f\"| Agglomerative | {best_k_agg} | {agg_sil:.3f} | Ward linkage |\\n\\n\")\n",
    "\n",
    "    fp.write(\"## Cluster Examples (K‑Means)\\n\")\n",
    "    for cid, samples in examples_per_cluster.items():\n",
    "        fp.write(f\"### Cluster {cid}\\n\")\n",
    "        for s in samples:\n",
    "            fp.write(f\"* {s}\\n\")\n",
    "        fp.write(\"\\n\")\n",
    "\n",
    "    fp.write(\"---\\n\")\n",
    "    fp.write(\"**Homogeneity** was _not_ computed because the dataset does not contain ground‑truth genre labels.\\n\")\n",
    "\n",
    "print(f\"Saved visualisations in {fig_dir}/ and report at {report_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
